# Progressive Model Inheritance Training Configuration
# Qwen3-VL-8B on RunPod A100
# Author: DeepALL Agent (2026)

# ============================================================================
# MODEL CONFIGURATION - QWEN3-VL-8B
# ============================================================================

model:
  # Base model: Qwen3-VL-8B (Vision Language)
  name: "Qwen/Qwen3-VL-8B"
  type: "vision-language"
  vocab_size: 152064                   # Qwen vocabulary size
  hidden_size: 3584                    # Qwen3-VL hidden size
  num_layers: 32                       # Qwen3-VL transformer layers
  num_heads: 32                        # Qwen3-VL attention heads
  head_dim: 112                        # Head dimension
  feedforward_dim: 15360               # Qwen3-VL feedforward dimension
  context_length: 256000               # 256K context window
  
  # Vision configuration
  vision_enabled: true
  vision_hidden_size: 3584
  image_size: 1024
  patch_size: 14
  num_vision_layers: 32
  
  # Miniseries sizes (d10 to d20)
  miniseries:
    d10:
      hidden_size: 768
      num_layers: 12
      num_params: 7000000              # ~7M parameters
    d11:
      hidden_size: 768
      num_layers: 12
      num_params: 14000000             # ~14M parameters
    d12:
      hidden_size: 768
      num_layers: 12
      num_params: 28000000             # ~28M parameters
    d13:
      hidden_size: 768
      num_layers: 12
      num_params: 56000000             # ~56M parameters
    d14:
      hidden_size: 768
      num_layers: 12
      num_params: 112000000            # ~112M parameters
    d15:
      hidden_size: 768
      num_layers: 12
      num_params: 224000000            # ~224M parameters
    d16:
      hidden_size: 768
      num_layers: 12
      num_params: 448000000            # ~448M parameters
    d17:
      hidden_size: 768
      num_layers: 12
      num_params: 896000000            # ~896M parameters
    d18:
      hidden_size: 3584
      num_layers: 32
      num_params: 8000000000           # ~8B parameters (Qwen3-VL-8B)

# ============================================================================
# TRAINING CONFIGURATION - OPTIMIZED FOR RUNPOD A100
# ============================================================================

training:
  # Basic training parameters (optimized for A100)
  num_epochs: 3                        # Epochs per model (reduced for cost)
  batch_size: 2                        # Per GPU batch size (A100 can handle more)
  gradient_accumulation_steps: 4       # Effective batch size: 2*4 = 8
  learning_rate: 1e-4                  # Learning rate for Qwen3-VL
  weight_decay: 0.01                   # L2 regularization
  
  # Gradient handling
  max_grad_norm: 1.0                   # Gradient clipping
  warmup_steps: 500                    # Warmup steps
  
  # Checkpointing
  checkpoint_interval: 500             # Save checkpoint every N batches
  save_best_model: true                # Save best validation model
  
  # Validation
  validation_split: 0.1                # 10% validation data
  eval_interval: 500                   # Evaluate every N batches
  
  # Mixed precision (important for A100)
  use_amp: true                        # Automatic Mixed Precision
  amp_dtype: "float16"                 # float16 or bfloat16

# ============================================================================
# SCALING LAWS CONFIGURATION
# ============================================================================

scaling_laws:
  tokens_per_model:
    d10: 56000000                      # 56M tokens for 7M params
    d11: 112000000                     # 112M tokens for 14M params
    d12: 224000000                     # 224M tokens for 28M params
    d13: 448000000                     # 448M tokens for 56M params
    d14: 896000000                     # 896M tokens for 112M params
    d15: 1792000000                    # 1.8B tokens for 224M params
    d16: 3584000000                    # 3.6B tokens for 448M params
    d17: 7168000000                    # 7.2B tokens for 896M params
    d18: 14336000000                   # 14.4B tokens for 8B params
  
  # Cost estimation (A100 GPU at $0.44/hour, RunPod)
  estimated_cost_usd:
    d10: 0.44
    d11: 0.44
    d12: 0.44
    d13: 0.44
    d14: 0.44
    d15: 0.44
    d16: 0.44
    d17: 0.44
    d18: 0.44
  
  total_estimated_cost: 3.96           # Total ~$3.96 for full miniseries (d10-d18)

# ============================================================================
# ARS OPTIMIZER CONFIGURATION
# ============================================================================

ars_optimizer:
  # Entropy Guard (Ψ_t) - Periodicity Detection
  entropy_window: 10                   # Window size for autocorrelation
  entropy_threshold: 0.5               # Periodicity detection threshold
  
  # Surprise Gate (Φ_t) - Gradient Damping
  surprise_window: 10                  # Window size for mean calculation
  surprise_threshold: 0.5              # Surprise detection threshold
  surprise_damping_strength: 0.1       # How much to dampen gradients
  
  # Chronos-Jitter (χ_t) - Pattern Breaking
  jitter_strength: 0.01                # Noise magnitude (relative to LR)
  jitter_seed: 42                      # Random seed for reproducibility
  
  # Expected benefits
  stability_improvement: 0.369         # 36.9% improvement
  convergence_speedup: 0.179           # 17.9% faster convergence
  recovery_improvement: 0.580          # 58% better recovery from spikes

# ============================================================================
# PROGRESSIVE INHERITANCE REGULARIZATION
# ============================================================================

regularization:
  # Fisher Information Matrix
  fisher_computation_batches: 100      # Batches for Fisher computation
  fisher_normalization: true           # Normalize by number of samples
  
  # Regularization strength (γ)
  gamma_initial: 0.1                   # Initial regularization strength
  gamma_decay_power: 1.0               # Linear decay (1.0) or quadratic (2.0)
  
  # L2 regularization
  l2_strength: 1e-5                    # Standard L2 regularization
  l2_weight: 0.0                       # Weight for L2 term (0 = disabled)
  
  # Expected benefits
  forgetting_prevention: 0.95          # 95% knowledge retention
  quality_improvement: 0.20            # 20% quality improvement

# ============================================================================
# DATA CONFIGURATION - RUNPOD PATHS
# ============================================================================

data:
  # Data paths (RunPod)
  train_data_path: "/root/data/training_data"
  val_data_path: "/root/data/training_data"
  test_data_path: "/root/data/training_data"
  
  # Data loading
  num_workers: 4                       # DataLoader workers
  pin_memory: true                     # Pin memory for faster loading
  prefetch_factor: 2                   # Prefetch batches
  
  # Preprocessing
  max_seq_length: 512                  # Maximum sequence length
  padding: "max_length"                # Padding strategy
  truncation: true                     # Truncate long sequences

# ============================================================================
# DEVICE CONFIGURATION - RUNPOD A100
# ============================================================================

device:
  # Hardware (RunPod A100)
  device_type: "cuda"                  # "cuda" or "cpu"
  device_id: 0                         # GPU device ID
  
  # Memory optimization for A100
  gradient_checkpointing: true         # Enable gradient checkpointing (saves memory)
  flash_attention: true                # Use flash attention (faster)
  
  # Distributed training
  distributed: false                   # Multi-GPU training (single A100)
  num_gpus: 1                          # Number of GPUs
  
  # A100 specific optimizations
  max_memory_percentage: 0.9           # Use 90% of A100 VRAM (80GB)

# ============================================================================
# LOGGING AND MONITORING
# ============================================================================

logging:
  # Log levels
  log_level: "INFO"                    # "DEBUG", "INFO", "WARNING", "ERROR"
  log_file: "/root/outputs/logs/training.log"
  
  # Metrics tracking
  log_interval: 10                     # Log every N batches
  tensorboard_dir: "/root/outputs/tensorboard"
  
  # Metrics to track
  metrics:
    - train_loss
    - val_loss
    - ars_damping
    - ars_entropy
    - ars_surprise
    - ars_jitter
    - weight_changes
    - learning_rate

# ============================================================================
# CHECKPOINT AND SAVING - RUNPOD PATHS
# ============================================================================

checkpointing:
  # Checkpoint paths (RunPod)
  checkpoint_dir: "/root/models/checkpoints"
  weights_dir: "/root/models/weights"
  fisher_dir: "/root/models/fisher"
  
  # Checkpoint strategy
  save_optimizer_state: true           # Save optimizer state
  save_scheduler_state: true           # Save scheduler state
  save_ars_state: true                 # Save ARS state
  
  # Keep best N checkpoints
  keep_best_n: 3
  keep_last_n: 2

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================

evaluation:
  # Evaluation metrics
  compute_perplexity: true
  compute_core_score: true             # CORE score from DCLM paper
  
  # CORE score configuration
  core_sample_size: 1000               # Samples for CORE computation
  
  # Scaling law validation
  validate_scaling_laws: true
  plot_scaling_curves: true

# ============================================================================
# ADVANCED CONFIGURATION
# ============================================================================

advanced:
  # Seed for reproducibility
  seed: 42
  
  # Debugging
  debug_mode: false
  profile_memory: false
  
  # Experimental features
  use_experimental_features: false
  
  # Custom callbacks
  callbacks:
    - "early_stopping"
    - "learning_rate_scheduler"
    - "checkpoint_saver"
  
  # RunPod specific
  runpod_mode: true
  save_intermediate_models: true
  use_huggingface_hub: true
