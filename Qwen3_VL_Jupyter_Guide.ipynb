{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-VL-8B - Complete Jupyter Notebook Guide\n",
    "\n",
    "**Ziel**: Qwen3-VL-8B Modell herunterladen, laden und verwenden\n",
    "\n",
    "**Anforderungen**:\n",
    "- GPU mit mindestens 16GB VRAM (besser 24GB+)\n",
    "- Hugging Face Account (optional, aber empfohlen)\n",
    "- Internet-Verbindung für Download (~16GB)\n",
    "\n",
    "**Zeit**: ~30 Minuten für Download + Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 1: Pakete installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installieren Sie die notwendigen Pakete\n",
    "!pip install --upgrade transformers torch torchvision torchaudio\n",
    "!pip install accelerate bitsandbytes peft pillow\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 2: Hugging Face Login (Optional aber empfohlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Token eingeben\n",
    "# Gehen Sie zu https://huggingface.co/settings/tokens und erstellen Sie einen Token\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interaktiv (wird Sie auffordern, Token einzugeben)\n",
    "login()\n",
    "\n",
    "# Option 2: Mit Token direkt\n",
    "# login(token=\"hf_xxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 3: GPU-Status prüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ CUDA not available! Model loading will be slow.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 4: Modell und Tokenizer herunterladen\n",
    "\n",
    "⚠️ **Warnung**: Dies wird ~16GB herunterladen und dauert 10-30 Minuten je nach Internetgeschwindigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-8B\"\n",
    "\n",
    "print(f\"Downloading {model_name}...\")\n",
    "print(\"This may take 10-30 minutes depending on your internet speed.\")\n",
    "print()\n",
    "\n",
    "# Tokenizer herunterladen\n",
    "print(\"Step 1: Downloading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"✓ Tokenizer downloaded successfully\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print()\n",
    "\n",
    "# Modell herunterladen\n",
    "print(\"Step 2: Downloading model (this is the large part)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Speichern als float16 um Speicher zu sparen\n",
    "    device_map=\"auto\",  # Automatisch auf GPU laden\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"✓ Model downloaded successfully\")\n",
    "print(f\"  Parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"  Model size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✓ MODEL AND TOKENIZER READY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 5: Speicher-Optimierung (Falls nötig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls Sie Speicherprobleme haben, verwenden Sie Quantization\n",
    "# Dies reduziert Speicher um ~75% mit minimalem Qualitätsverlust\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 4-bit Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Modell mit Quantization laden (optional)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"Qwen/Qwen3-VL-8B\",\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "print(\"Quantization config prepared (optional)\")\n",
    "print(\"Uncomment the code above if you need to save memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 6: Einfacher Text-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfache Text-Generierung\n",
    "\n",
    "prompt = \"What is machine learning?\"\n",
    "\n",
    "# Text tokenisieren\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generieren\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "# Dekodieren\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROMPT:\")\n",
    "print(prompt)\n",
    "print()\n",
    "print(\"RESPONSE:\")\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 7: Vision-Fähigkeiten testen (mit Bild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-Fähigkeiten: Bild beschreiben\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Beispiel: Bild von URL laden\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "\n",
    "try:\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    \n",
    "    # Prompt mit Bild\n",
    "    prompt = \"Describe this image in detail.\"\n",
    "    \n",
    "    # Für Vision-Modelle brauchen wir spezielle Verarbeitung\n",
    "    # (Dies ist ein Beispiel - die genaue Implementierung hängt vom Modell ab)\n",
    "    \n",
    "    print(\"Image loaded successfully\")\n",
    "    print(f\"Image size: {image.size}\")\n",
    "    print()\n",
    "    print(\"Note: Vision processing requires model-specific preprocessing.\")\n",
    "    print(\"Check Qwen3-VL documentation for exact vision API.\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    print(\"You can use local images instead:\")\n",
    "    print(\"  image = Image.open('/path/to/your/image.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 8: Batch-Inferenz (mehrere Prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrere Prompts auf einmal verarbeiten\n",
    "\n",
    "prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain neural networks\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts...\")\n",
    "print()\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"[{i}/{len(prompts)}] {prompt}\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Response: {response[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 9: Speicher-Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MEMORY USAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU Memory\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        \n",
    "        print(f\"  GPU {i}:\")\n",
    "        print(f\"    Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"    Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"    Total: {total:.2f} GB\")\n",
    "        print(f\"    Free: {total - allocated:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU Memory: Not available\")\n",
    "\n",
    "# CPU Memory\n",
    "print()\n",
    "print(\"CPU Memory:\")\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"  Used: {memory.used / 1e9:.2f} GB\")\n",
    "print(f\"  Available: {memory.available / 1e9:.2f} GB\")\n",
    "print(f\"  Total: {memory.total / 1e9:.2f} GB\")\n",
    "print(f\"  Percent: {memory.percent}%\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 10: Modell speichern (lokal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell lokal speichern um es später schneller zu laden\n",
    "\n",
    "save_path = \"./qwen3-vl-8b-local\"\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "\n",
    "# Modell speichern\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"✓ Model saved successfully\")\n",
    "print()\n",
    "print(\"Later, you can load it faster with:\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{save_path}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{save_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 11: Modell aus lokaler Kopie laden (schneller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls Sie das Modell später wieder laden wollen (schneller als Download)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "save_path = \"./qwen3-vl-8b-local\"\n",
    "\n",
    "print(f\"Loading model from {save_path}...\")\n",
    "\n",
    "# Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path, trust_remote_code=True)\n",
    "\n",
    "# Modell laden\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully from local copy\")\n",
    "print(f\"  Parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 12: Cleanup (Speicher freigeben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls Sie mit dem Modell fertig sind und Speicher freigeben möchten\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up...\")\n",
    "\n",
    "# Modell und Tokenizer löschen\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# GPU Cache leeren\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✓ Memory cleaned up\")\n",
    "\n",
    "# Speicher prüfen\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    print(f\"GPU memory allocated: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipps & Tricks\n",
    "\n",
    "### Download-Optionen:\n",
    "```python\n",
    "# Option 1: Automatisch auf GPU laden (empfohlen)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-8B\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Option 2: Auf CPU laden (langsamer, aber weniger VRAM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-8B\",\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "# Option 3: Mit Quantization (speichersparend)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-8B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Speicher-Anforderungen:\n",
    "- **Full Precision (float32)**: ~32GB VRAM\n",
    "- **Half Precision (float16)**: ~16GB VRAM\n",
    "- **4-bit Quantization**: ~4GB VRAM\n",
    "\n",
    "### Download-Speicherort:\n",
    "```bash\n",
    "# Standard: ~/.cache/huggingface/hub/\n",
    "# Ändern mit:\n",
    "export HF_HOME=/custom/path\n",
    "```\n",
    "\n",
    "### Schnellerer Download:\n",
    "```bash\n",
    "# Mit git-lfs (falls installiert)\n",
    "git clone https://huggingface.co/Qwen/Qwen3-VL-8B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem: \"CUDA out of memory\"\n",
    "**Lösung**: \n",
    "- Verwenden Sie float16 statt float32\n",
    "- Verwenden Sie 4-bit Quantization\n",
    "- Reduzieren Sie batch_size\n",
    "\n",
    "### Problem: \"Model not found\"\n",
    "**Lösung**:\n",
    "- Login zu Hugging Face: `huggingface-cli login`\n",
    "- Prüfen Sie Internetverbindung\n",
    "- Versuchen Sie mit `trust_remote_code=True`\n",
    "\n",
    "### Problem: \"Slow download\"\n",
    "**Lösung**:\n",
    "- Speichern Sie das Modell lokal nach dem Download\n",
    "- Verwenden Sie einen besseren Internet-Connection\n",
    "- Versuchen Sie zu einer anderen Zeit\n",
    "\n",
    "### Problem: \"Kernel crashes\"\n",
    "**Lösung**:\n",
    "- Starten Sie den Kernel neu\n",
    "- Reduzieren Sie max_length in generate()\n",
    "- Verwenden Sie Quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
