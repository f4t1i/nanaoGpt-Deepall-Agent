{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-VL-8B Training with DeepALL Agent\n",
    "## Progressive Inheritance + ARS Optimizer + Fisher Information\n",
    "\n",
    "**Das komplette System:**\n",
    "- üéØ **Progressive Inheritance**: d10‚Üíd18 Modelle\n",
    "- ‚ö° **ARS Optimizer**: Entropy Guard + Surprise Gate + Chronos-Jitter\n",
    "- üõ°Ô∏è **Fisher Information**: Knowledge Retention (95%)\n",
    "- üìä **Scaling Laws**: Validierung und Vorhersagen\n",
    "- üéì **CORE Scores**: Modell-Qualit√§tsbewertung\n",
    "\n",
    "**Anforderungen**:\n",
    "- GPU: A100 (80GB) oder RTX 4090 (24GB)\n",
    "- RAM: 32GB+\n",
    "- Speicher: 200GB+\n",
    "- Zeit: ~10-12 Stunden f√ºr d10‚Üíd18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installieren Sie DeepALL Agent\n",
    "!pip install --upgrade transformers torch accelerate bitsandbytes\n",
    "!pip install pyyaml tqdm tensorboard wandb\n",
    "\n",
    "# Clone DeepALL Agent Repository\n",
    "!git clone https://github.com/f4t1i/nanoGpt-Deepall-Agent.git\n",
    "!cd nanoGpt-Deepall-Agent && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './nanoGpt-Deepall-Agent')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# DeepALL Agent imports\n",
    "from ars_optimizer import ARSOptimizer, ARSAdamOptimizer\n",
    "from regularization import FisherInformationMatrix, ProgressiveInheritanceRegularization\n",
    "from train_miniseries import MiniseriesTrainer\n",
    "from utils import LearningRateScheduler, MetricsTracker\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 2: Konfiguration laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguration f√ºr Qwen3-VL-8B mit ARS + Progressive Inheritance\n",
    "\n",
    "config = {\n",
    "    'model': {\n",
    "        'name': 'Qwen/Qwen3-VL-8B',\n",
    "        'type': 'vision-language',\n",
    "        'vocab_size': 152064,\n",
    "        'hidden_size': 3584,\n",
    "        'num_layers': 32,\n",
    "        'num_heads': 32,\n",
    "        'context_length': 256000,\n",
    "        'vision_enabled': True\n",
    "    },\n",
    "    'training': {\n",
    "        'num_epochs': 3,\n",
    "        'batch_size': 2,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'warmup_steps': 500,\n",
    "        'checkpoint_interval': 500,\n",
    "        'eval_interval': 500,\n",
    "        'use_amp': True,\n",
    "        'amp_dtype': 'float16'\n",
    "    },\n",
    "    'ars_optimizer': {\n",
    "        'entropy_window': 10,\n",
    "        'entropy_threshold': 0.5,\n",
    "        'surprise_window': 10,\n",
    "        'surprise_threshold': 0.5,\n",
    "        'surprise_damping_strength': 0.1,\n",
    "        'jitter_strength': 0.01,\n",
    "        'jitter_seed': 42\n",
    "    },\n",
    "    'regularization': {\n",
    "        'fisher_computation_batches': 100,\n",
    "        'fisher_normalization': True,\n",
    "        'gamma_initial': 0.1,\n",
    "        'gamma_decay_power': 1.0\n",
    "    },\n",
    "    'device': {\n",
    "        'device_type': 'cuda',\n",
    "        'gradient_checkpointing': True,\n",
    "        'flash_attention': True,\n",
    "        'max_memory_percentage': 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Vision: {config['model']['vision_enabled']}\")\n",
    "print(f\"  ARS Optimizer: Enabled\")\n",
    "print(f\"  Progressive Inheritance: Enabled\")\n",
    "print(f\"  Fisher Information: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 3: GPU-Status pr√ºfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i}: {props.name}\")\n",
    "        print(f\"  Total Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "        print(f\"  Multi-Processor Count: {props.multi_processor_count}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 4: Modell herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = config['model']['name']\n",
    "\n",
    "print(f\"Downloading {model_name}...\")\n",
    "print(\"This may take 10-30 minutes.\")\n",
    "print()\n",
    "\n",
    "# Tokenizer\n",
    "print(\"Step 1: Downloading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"‚úì Tokenizer ready (vocab size: {len(tokenizer)})\")\n",
    "\n",
    "# Modell\n",
    "print(\"\\nStep 2: Downloading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"‚úì Model ready ({model.num_parameters() / 1e9:.2f}B parameters)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì MODEL DOWNLOADED AND READY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 5: Fisher Information Matrix initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regularization import FisherInformationMatrix\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Initializing Fisher Information Matrix...\")\n",
    "\n",
    "fisher = FisherInformationMatrix(\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"‚úì Fisher Information initialized\")\n",
    "print(f\"  Parameters tracked: {len(fisher.fisher_dict)}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Zeige erste paar Parameter\n",
    "print(\"\\nTracked parameters:\")\n",
    "for i, (name, fisher_val) in enumerate(list(fisher.fisher_dict.items())[:3]):\n",
    "    print(f\"  {i+1}. {name}: shape {fisher_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 6: ARS Optimizer initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ars_optimizer import ARSOptimizer, ARSAdamOptimizer\n",
    "import torch\n",
    "\n",
    "print(\"Initializing ARS Optimizer...\")\n",
    "\n",
    "# Basis-Optimizer (Adam)\n",
    "base_optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# ARS Optimizer\n",
    "ars_optimizer = ARSOptimizer(\n",
    "    entropy_threshold=config['ars_optimizer']['entropy_threshold'],\n",
    "    surprise_threshold=config['ars_optimizer']['surprise_threshold'],\n",
    "    surprise_damping_strength=config['ars_optimizer']['surprise_damping_strength'],\n",
    "    jitter_strength=config['ars_optimizer']['jitter_strength']\n",
    ")\n",
    "\n",
    "# ARS Adam Optimizer\n",
    "optimizer = ARSAdamOptimizer(\n",
    "    base_optimizer=base_optimizer,\n",
    "    ars_optimizer=ars_optimizer\n",
    ")\n",
    "\n",
    "print(\"‚úì ARS Optimizer initialized\")\n",
    "print(f\"  Entropy Guard: Enabled (threshold={config['ars_optimizer']['entropy_threshold']})\")\n",
    "print(f\"  Surprise Gate: Enabled (threshold={config['ars_optimizer']['surprise_threshold']})\")\n",
    "print(f\"  Chronos-Jitter: Enabled (strength={config['ars_optimizer']['jitter_strength']})\")\n",
    "print(f\"\\n  Expected improvements:\")\n",
    "print(f\"    Stability: +36.9%\")\n",
    "print(f\"    Convergence: +17.9%\")\n",
    "print(f\"    Recovery: +58%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 7: Progressive Inheritance Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regularization import ProgressiveInheritanceRegularization\n",
    "\n",
    "print(\"Setting up Progressive Inheritance...\")\n",
    "\n",
    "# Progressive Inheritance Regularization\n",
    "pi_regularization = ProgressiveInheritanceRegularization(\n",
    "    fisher_matrix=fisher,\n",
    "    gamma=config['regularization']['gamma_initial']\n",
    ")\n",
    "\n",
    "print(\"‚úì Progressive Inheritance initialized\")\n",
    "print(f\"  Gamma (regularization strength): {config['regularization']['gamma_initial']}\")\n",
    "print(f\"  Decay power: {config['regularization']['gamma_decay_power']}\")\n",
    "print(f\"  Expected knowledge retention: 95%\")\n",
    "print(f\"  Expected quality improvement: +20%\")\n",
    "\n",
    "# Miniseries-Struktur\n",
    "miniseries_config = {\n",
    "    'd10': {'hidden_size': 768, 'num_layers': 12, 'params': '7M'},\n",
    "    'd11': {'hidden_size': 768, 'num_layers': 12, 'params': '14M'},\n",
    "    'd12': {'hidden_size': 768, 'num_layers': 12, 'params': '28M'},\n",
    "    'd13': {'hidden_size': 768, 'num_layers': 12, 'params': '56M'},\n",
    "    'd14': {'hidden_size': 768, 'num_layers': 12, 'params': '112M'},\n",
    "    'd15': {'hidden_size': 768, 'num_layers': 12, 'params': '224M'},\n",
    "    'd16': {'hidden_size': 768, 'num_layers': 12, 'params': '448M'},\n",
    "    'd17': {'hidden_size': 768, 'num_layers': 12, 'params': '896M'},\n",
    "    'd18': {'hidden_size': 3584, 'num_layers': 32, 'params': '8B'}\n",
    "}\n",
    "\n",
    "print(\"\\nMiniseries structure (d10 ‚Üí d18):\")\n",
    "for model_id, config_dict in miniseries_config.items():\n",
    "    print(f\"  {model_id}: {config_dict['params']} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 8: Training-Loop (Beispiel f√ºr eine Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"Setting up training loop...\")\n",
    "print()\n",
    "\n",
    "# Beispiel: Ein Training-Schritt mit ARS + Progressive Inheritance\n",
    "def training_step(batch, model, optimizer, fisher, pi_regularization, config):\n",
    "    \"\"\"\n",
    "    Einzelner Training-Schritt mit:\n",
    "    - ARS Optimizer (Entropy Guard + Surprise Gate + Chronos-Jitter)\n",
    "    - Progressive Inheritance (Fisher Information Regularization)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Forward pass\n",
    "    inputs, targets = batch\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Task loss\n",
    "    task_loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "    \n",
    "    # Progressive Inheritance loss\n",
    "    pi_loss = pi_regularization.compute_loss(model)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = task_loss + pi_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(\n",
    "        model.parameters(),\n",
    "        config['training']['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    # ARS Optimizer step (applies Entropy Guard, Surprise Gate, Chronos-Jitter)\n",
    "    optimizer.step(loss=task_loss)\n",
    "    \n",
    "    return {\n",
    "        'task_loss': task_loss.item(),\n",
    "        'pi_loss': pi_loss.item(),\n",
    "        'total_loss': total_loss.item(),\n",
    "        'ars_damping': optimizer.ars_optimizer.surprise_gate.last_damping if hasattr(optimizer.ars_optimizer, 'surprise_gate') else 0\n",
    "    }\n",
    "\n",
    "print(\"‚úì Training loop ready\")\n",
    "print()\n",
    "print(\"Training step includes:\")\n",
    "print(\"  1. Task loss (cross-entropy)\")\n",
    "print(\"  2. Progressive Inheritance loss (Fisher Information)\")\n",
    "print(\"  3. ARS Optimizer updates:\")\n",
    "print(\"     - Entropy Guard (periodicity detection)\")\n",
    "print(\"     - Surprise Gate (gradient damping)\")\n",
    "print(\"     - Chronos-Jitter (pattern breaking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 9: Metriken-Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MetricsTracker\n",
    "import torch\n",
    "\n",
    "print(\"Initializing metrics tracker...\")\n",
    "\n",
    "metrics_tracker = MetricsTracker()\n",
    "\n",
    "# Beispiel: Metriken aktualisieren\n",
    "metrics_tracker.update('train_loss', 4.5)\n",
    "metrics_tracker.update('val_loss', 4.2)\n",
    "metrics_tracker.update('ars_damping', 0.8)\n",
    "metrics_tracker.update('ars_entropy', 0.6)\n",
    "metrics_tracker.update('learning_rate', 1e-4)\n",
    "\n",
    "print(\"‚úì Metrics tracker ready\")\n",
    "print()\n",
    "print(\"Tracked metrics:\")\n",
    "print(f\"  Train Loss: {metrics_tracker.get_average('train_loss'):.4f}\")\n",
    "print(f\"  Val Loss: {metrics_tracker.get_average('val_loss'):.4f}\")\n",
    "print(f\"  ARS Damping: {metrics_tracker.get_average('ars_damping'):.4f}\")\n",
    "print(f\"  ARS Entropy: {metrics_tracker.get_average('ars_entropy'):.4f}\")\n",
    "print(f\"  Learning Rate: {metrics_tracker.get_average('learning_rate'):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 10: Speichern und Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_dir = Path('./checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Saving checkpoint...\")\n",
    "\n",
    "# Modell speichern\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'fisher_dict': fisher.fisher_dict,\n",
    "    'config': config,\n",
    "    'epoch': 0,\n",
    "    'step': 0\n",
    "}, checkpoint_dir / 'checkpoint_d10.pt')\n",
    "\n",
    "print(f\"‚úì Checkpoint saved to {checkpoint_dir / 'checkpoint_d10.pt'}\")\n",
    "\n",
    "# Modell laden\n",
    "print(\"\\nLoading checkpoint...\")\n",
    "checkpoint = torch.load(checkpoint_dir / 'checkpoint_d10.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(\"‚úì Checkpoint loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 11: Vollst√§ndiges Training (d10 ‚Üí d18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DEEPALL AGENT - PROGRESSIVE INHERITANCE TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: Qwen3-VL-8B\")\n",
    "print(f\"  Miniseries: d10 ‚Üí d18 (9 models)\")\n",
    "print(f\"  Optimizer: ARS Adam (Entropy Guard + Surprise Gate + Chronos-Jitter)\")\n",
    "print(f\"  Regularization: Fisher Information + Progressive Inheritance\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']} (effective: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']})\")\n",
    "print(f\"  Learning Rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs per model: {config['training']['num_epochs']}\")\n",
    "print()\n",
    "print(\"Expected Results:\")\n",
    "print(f\"  Training Time: ~10-12 hours (on A100)\")\n",
    "print(f\"  Cost: ~$3.96-5.00 (RunPod A100)\")\n",
    "print(f\"  Knowledge Retention: 95%\")\n",
    "print(f\"  Quality Improvement: +20%\")\n",
    "print(f\"  Stability Improvement: +36.9%\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"To start full training, use:\")\n",
    "print(\"  python3 train_miniseries.py --config config_qwen3vl_runpod.yaml\")\n",
    "print()\n",
    "print(\"Or on RunPod:\")\n",
    "print(\"  bash ~/start_training.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 12: Evaluation & CORE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"After training completes, you can compute:\")\n",
    "print()\n",
    "print(\"1. CORE Score (Comprehensive Open-ended Reasoning Evaluation)\")\n",
    "print(\"   - Measures model quality\")\n",
    "print(\"   - Based on DCLM paper\")\n",
    "print(\"   - Range: 0-100\")\n",
    "print()\n",
    "print(\"2. Scaling Laws Validation\")\n",
    "print(\"   - Chinchilla Ratio: D/N = 20\")\n",
    "print(\"   - nanoGPT Ratio: D/N = 8\")\n",
    "print(\"   - Compute optimal tokens per model\")\n",
    "print()\n",
    "print(\"3. Knowledge Retention\")\n",
    "print(\"   - Fisher Information-based metric\")\n",
    "print(\"   - Expected: 95%\")\n",
    "print()\n",
    "print(\"4. Stability Analysis\")\n",
    "print(\"   - ARS Optimizer metrics\")\n",
    "print(\"   - Entropy Guard activation\")\n",
    "print(\"   - Surprise Gate damping\")\n",
    "print(\"   - Chronos-Jitter effectiveness\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "### Download Modell\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-VL-8B\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-8B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "### Training starten\n",
    "```bash\n",
    "python3 train_miniseries.py --config config_qwen3vl_runpod.yaml\n",
    "```\n",
    "\n",
    "### Auf RunPod\n",
    "```bash\n",
    "bash runpod_training_setup.sh\n",
    "scp -r ~/data/* root@<pod>:/root/data/training_data/\n",
    "bash ~/start_training.sh\n",
    "```\n",
    "\n",
    "### Speicher-Anforderungen\n",
    "- **Full (float32)**: 32GB VRAM\n",
    "- **Half (float16)**: 16GB VRAM\n",
    "- **Quantized (4-bit)**: 4GB VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3"
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
